---
layout: page
title: Multilingual Universal Sentence Encoder for Semantic Retrieval
comments: false
tags: [blog, Evgeniia-Razumovskaia, research, NLP, dialogue systems, welcome, phd-life, phd]
---


Yes, it's a weird <b>retro</b>spective which starts with a paper of year 2019. I guess it shows how quickly the field is moving. 
<br/>
<br/>

<b>Preface</b>
<br/>

  üî≤    Existing large language models such as BERT obtain state-of-the-art performance on many natural language understanding tasks. They are pretrained to create contextual word embeddings and sentence embeddings via masked language modelling and next sentence prediction. 
<br/>
  üî≤    In monolingual settings they are very effective for both word-level and sentence level tasks. Multilingual representations of sentences were not as effective in cross-lingual settings. 

<br/>
<br/>
<b>Problem</b>
<br/>
  üåé    For multilingual retrieval tasks, we require goood universal sentence encoder. By universal here I mean one that can capture semantics correctly in multiple domains in several languages. 
<br/>
  ‚ùî    How can we train such a universal encoder? Is there a possibility to make it more lightweight than its predecessors? 

<br/>
<br/>
<b>What is the solution proposed?</b>
<br/>

<i>Model</i>
<br/>
<img src="/../../images/posts/mUSE/model_encoder.png" style="max-width:55%; margin-left:20px;float:left;"/>
<br/>
The model architecture is as shown in the picture above. It is a <i>dual encoder</i> model: one side encodes the query, e.g., the question in the QA task, and the other side encodes all possible candidates, e.g., all possible responses in the QA tasks. The model computes a similarity metric between the query encoding and the response encodings. The output of the model is the response most similar to the query. 
<br/>